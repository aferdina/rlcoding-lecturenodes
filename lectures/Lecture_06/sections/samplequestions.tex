\section{Sample questions for exam}

\begin{frame}{Question 1}
    \textbf{Topic:} Optimising Reinforcement Learning Models: Callbacks

    \vspace{10pt}

    What is a callback in the context of training reinforcement learning models and how does its implementation work?

    \vspace{20pt}

    \textbf{Answer:} A \href{https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html}{callback} is a set of functions that will be called at given stages of the training procedure. You can use callbacks to access internal state of the RL model during training. It allows one to do monitoring, auto saving, model manipulation, progress bars, etc.
    lectures/Lecture\_06/images/Callback.drawio

\end{frame}

\begin{frame}{Question 2}
    \textbf{Topic:} Neural Networks in Reinforcement Learning
    \vspace{10pt}

    Give two examples of the use of neural networks in connection with reinforcment learning.

    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/ActorCritic.drawio
\end{frame}

\begin{frame}{Question 3}
    \textbf{Topic:} Implementing Neural Networks
    \vspace{10pt}

    Show schematically how the implementation of neural networks works in the context of the game environment in reinforcement learning. Name two additional parameters that can/must be defined for the creation of a neural network. 
\end{frame}
\begin{frame}
    \textbf{Answer:}
    \begin{enumerate}
        \item \textbf{Feature extrator for policy and value function:} The information about the observation space dimension is used to create a feature extractor for the policy and value function. Furthermore the type of feature extractor for the observation can be chosen (e.g flatten, convolutional, mlp, lstm, \dots). The result are neural net models of the form
        \[pi_{\text{feature}}: \mathbb{R}^{n_{\text{observation}}} \rightarrow \mathbb{R}^{n_{pi\text{features}}},\] and
        \[vi_{\text{feature}}: \mathbb{R}^{n_{\text{observation}}} \rightarrow \mathbb{R}^{n_{vi\text{features}}}\]
    \end{enumerate}
\end{frame}
\begin{frame}
    \textbf{Answer:} lectures/Lecture\_06/images/CreateNeuralNetwork.drawio
    \begin{enumerate}[2.]
        \item \textbf{MLP Extractor:} In the context of Stable Baselines, an MLP extractor refers to a type of neural network architecture used for feature extraction in policy networks. Stable Baselines3 provides different types of policy networks, including those for images (CnnPolicies), various input features (MlpPolicies), and multiple inputs (MultiInputPolicies). For a 1D observation space, a fully connected network with multiple layers is used, where the number of units per layer varies depending on the algorithm used. The result are neural net models of the form
        \[pi_{\text{fully con}}: \mathbb{R}^{n_{pi \text{features}}} \rightarrow \mathbb{R}^{n_{pi \text{latent dim}}},\] and
        \[vi_{\text{fully con}}: \mathbb{R}^{n_{vi \text{features}}} \rightarrow \mathbb{R}^{n_{vi \text{latent dim}}}\]
    \end{enumerate}
\end{frame}
\begin{frame}
    \textbf{Answer:}
    \begin{enumerate}[3.]
        \item \textbf{Value Action mapping:} The value function returns a real number. An additional layer is therefore required to map the latent space to a real number. For the action it is a bit more complicated. There the latent space is mapped to a distrubution over the action space. This is for example done by a normal distribution with mean $\mu$ and standard deviation $\sigma$ or a softmax function. The result are neural net models of the form
        \[pi_{\text{action}}: \mathbb{R}^{n_{pi \text{latent dim}}} \rightarrow \mathbb{R}^{n_{\text{action}}},\] and
        \[vi_{\text{value}}: \mathbb{R}^{n_{vi \text{latent dim}}} \rightarrow \mathbb{R}.\]
    \end{enumerate}
\end{frame}

\begin{frame}{Question 4}
    \textbf{Topic:} Monitoring Training Process: Tensorboard
    \vspace{10pt}

    What is a tensor board and what is it used for in the context of reinforcement learning? How is the information provided for the tensor board? 

    \vspace{20pt}

    \textbf{Answer:} TensorBoard is a visualization tool commonly used in machine learning to monitor and visualize the training process of deep learning models, including reinforcement learning models. Stable Baselines is a popular library for reinforcement learning in Python, and it provides built-in support for integrating TensorBoard to track and visualize the training progress of your reinforcement learning agents. In Stable Baselines, you can use the TensorboardCallback class to set up TensorBoard logging during training. The TensorboardCallback allows you to customize what information is logged to TensorBoard by specifying the relevant arguments when creating an instance of the callback. For example, you can set log frequency to control how often the logs are updated or use verbose to control the verbosity level of the logs.
\end{frame}

\begin{frame}{Question 6}
    \textbf{Topic:} Storing Reinforcement Learning Parameter
    \vspace{10pt}

    Name 3 different elements that are stored when training reinforcement learning models. Describe the functions of the individual elements.

    \vspace{20pt}

    \textbf{Answer:} 
    \begin{enumerate}
        \item \textbf{Model Parameters (Neural Network Weights) for the Policy:} The model parameters of the neural network represent the weights and biases that define the policy function. These parameters are learned and updated during the training process to improve the policy's performance.
    \end{enumerate}
\end{frame}
\begin{frame}
    \textbf{Answer:}
    \begin{enumerate}[2.]
        \item \textbf{Normalization Parameters for the Environment (Observation Scaling and Shifting):} The normalization parameters for the environment are used to scale and shift the observations to a range that is suitable for the neural network. This is done to improve the performance of the neural network.
        \item \textbf{TensorBoard Log File:} The TensorBoard log file is a record of training data and metrics that are used to visualize the training process in TensorBoard. It contains information such as training loss, rewards, policy updates, and other relevant statistics.
    \end{enumerate}
\end{frame}

\begin{frame}{Question 7}
    \textbf{Topic:} Modelling Reinforcment Learning: GridWorld
    \vspace{10pt}

    Name 5 components that are needed to model the game GridWorld from the lecture. Also describe the implementation of the individual components.  
    \vspace{20pt}

    \textbf{Answer:} 
    \begin{enumerate}
        \item Action space
        \item Observation space
        \item Reward function
        \item Transition function
        \item Initial state distribution
    \end{enumerate}
    See lecture slides and code for implementation details.
\end{frame}

\begin{frame}{Question 8}
    \textbf{Topic:} Implementing Multiarmed Bandit Environments
    \vspace{10pt}

    Create a UML class diagram for the implementation of a multi-armed bandit algorithm for an environment. Name all required methods and attributes. 
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/MABClassDiagramm.drawio
\end{frame}

\begin{frame}{Question 9}
    \textbf{Topic:} Implementing Reinforcment Learning Models
    \vspace{10pt}

    Which mathematical model is described by the Python package `Gym`/`Gymnasium`. Name two main components of the package and how they are used. 
    \vspace{20pt}

    \textbf{Answer:} See lecture 5
\end{frame}

\begin{frame}{Question 10}
    \textbf{Topic:} Updating Neural Networks
    \vspace{10pt}

    Create a flowchart for the training update of neural networks in `torch` in connection with an optimization function.
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/UpdatingNeuralNetworks.drawio
\end{frame}

\begin{frame}{Question 11}
    \textbf{Topic:} UML Class Diagram: Reinforcement Learning Algorithm
    \vspace{10pt}

    Create and describe a simplified UML class diagram for a Reinformcent Learning algorithm of your choice.
  The answer should include the following components:
  \begin{enumerate}
      \item The policy type of the algorithm
      \item The type of the algorithm
      \item The methods and components to train the algorithm
  \end{enumerate} 
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/RLClassDiagram.drawio
\end{frame}

\begin{frame}{Question 12}
    \textbf{Topic:} UML Class Diagram: Autonomous Driving
    \vspace{10pt}

    Create and describe a simplified UML class diagram for the autonomous driving game environment \glqq SimpleDriver\grqq.
    The answer should include the following components:
    \begin{enumerate}
        \item The observation space
        \item The action space
        \item The configuration of the environment
        \item The reward function
        \item The transition logic
    \end{enumerate}
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/PorscheAiClassDiagram.drawio
\end{frame}

\begin{frame}{Question 13}
    \textbf{Topic:} Flowchart Diagram for Optimizing Multi-Armed Bandit Algorithms
    \vspace{10pt}

    Create a flowchart for optimizing multi-armed bandit algorithms. 
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/TrainMABModels.drawio
\end{frame}

\begin{frame}{Question 14}
    \textbf{Topic:} Visualizing Reinforcement Learning Models
    \vspace{10pt}

    What is the Pygame package used for in the context of reinforcement learning and describe two features of the package. 
    \vspace{20pt}

    \textbf{Answer:}
    Pygame is a popular Python library that is primarily used for developing 2D video games and multimedia applications. It provides a simple and intuitive interface for handling graphics, user input, sound, and more. While pygame itself is not specifically designed for reinforcement learning (RL), it can be used in the context of RL for various purposes:
\end{frame}
\begin{frame}
    \begin{enumerate}
        \item Pygame can be used to create custom environments for reinforcement learning experiments. You can design 2D game environments with pygame where agents interact with the game world and learn to make decisions through RL algorithms. These custom environments can serve as testbeds for RL research and experimentation.
        \item Pygame can be used to simulate and visualize RL scenarios. You can use it to render and animate the agent's actions and the environment's dynamics. This visualization can be helpful for debugging RL algorithms and understanding the agent's behavior during training.
    \end{enumerate}
    
\end{frame}
\begin{frame}
    In Pygame, sprites are a fundamental concept used for creating and managing 2D graphical objects within a game or graphical application. Sprites are typically used to represent characters, objects, or other entities in the game world. They simplify the process of drawing and updating these objects on the screen.

Here are some key characteristics and concepts related to sprites in Pygame:

Sprite Groups: Sprites are often organized into groups, which can be helpful for managing and manipulating multiple sprites at once. Pygame's pygame.sprite.Group class allows you to create and manage groups of sprites efficiently. You can add, remove, update, and draw all sprites in a group with a single method call.
Each sprite has a position, usually represented as an (x, y) coordinate on the screen. Pygame provides the pygame.Rect class, which is commonly used to represent the bounding rectangle of a sprite. This rectangle defines the sprite's position and size and is useful for collision detection and other operations.
\end{frame}
\begin{frame}
    In a game loop, you typically update the state of each sprite (e.g., move, animate) and then render (draw) them on the screen. Pygame provides methods like blit to draw sprites onto the display surface. You can also use sprite groups to update and draw all sprites in a group efficiently.
    Pygame offers tools for collision detection between sprites. You can check if two sprites intersect by comparing their Rect objects. This is essential for handling interactions and collisions between game objects.
    Sprites are often used for animations. You can create animations by displaying different images (frames) in sequence to create the illusion of motion. Pygame provides functions and classes for handling sprite animations.
\end{frame}

\begin{frame}{Question 15}
    \textbf{Topic:} Tabular Methods
    \vspace{10pt}

    Create a simpliefied class diagram for the implementation of tabular methods from the lecture. What is the structure of the underlying decision rule? 
    \vspace{20pt}

    \textbf{Answer:} lectures/Lecture\_06/images/FiniteClasses.drawio
\end{frame}

\begin{frame}{Question 16-20}
    \textbf{Topic:} Solid Principles

    Describe each of the solid principles and explain the principle using an adequate example. 
    \vspace{20pt}

    \textbf{Answer:} see lecture 1 and example from introduction repo
\end{frame}

\begin{frame}{Question 21 \& 22}
    \textbf{Topic:} Multi-Armed Bandits: Metrics
    \vspace{10pt}
    Specify the definition of a metric for the evaluation of multi-armed bandits.
    \vspace{20pt}

    \textbf{Answer:} Regret and Failure Probability: See lecture slides
\end{frame}

% \begin{frame}{Question 23}
%     \textbf{Topic:} Implementing Reinforcement Learning Environments
    
%     \vspace{10pt}
%     Which methods and attributes need to be defined for a gym environment. Which mathematical concepts are behind the individual attributes. What are the methods used for? 
%     \vspace{20pt}

%     \textbf{Answer:} [Insert answer or grading criteria]
% \end{frame}

% \begin{frame}{Question 24}
%     \textbf{Topic:} Wrapper Classes and Reinforcement Learning Environments

%     \vspace{10pt}
%     How does a wrapper class work and what is it used for in the context of reinforcement learning environments?   
%     \vspace{20pt}

%     \textbf{Answer:} [Insert answer or grading criteria]
% \end{frame}

% \begin{frame}{Question 25}
%     \textbf{Topic:} Hyperameter Tuning

%     \vspace{10pt}
%     Name three building blocks for hyperparameter optimization in the context of reinformcent learning.  
%     \vspace{20pt}

%     \textbf{Answer:} [Insert answer or grading criteria]
% \end{frame}
% \begin{frame}{Question 26}
%     \textbf{Topic:} Policy Iteration Algorithm: Flow Diagram

%     \vspace{10pt}
%     Create a flowchart for the implementation of the algorithm. 
%     \vspace{20pt}

%     \textbf{Answer:} [Insert answer or grading criteria]
% \end{frame}

% \begin{frame}{Question 27}
%     \textbf{Topic:} Dynyamic Programming: Flow Diagram

%     \vspace{10pt}
%     Create a flowchart for the implementation of the algorithm. 
%     \vspace{20pt}

%     \textbf{Answer:} [Insert answer or grading criteria]
% \end{frame}

